"""The lambda function for the Bolt app."""
import json
import logging
import os
import re
import time
from typing import Any

import lancedb
import slack_bolt
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.memory.buffer import ConversationBufferMemory
from langchain_community.chat_message_histories import DynamoDBChatMessageHistory
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores.lancedb import LanceDB
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.outputs.llm_result import LLMResult
from langchain_openai.embeddings import OpenAIEmbeddings
from slack_bolt import Ack, BoltContext, Say
from slack_bolt.adapter.aws_lambda import SlackRequestHandler
from slack_sdk.models.blocks import (
    ContextBlock,
    DividerBlock,
    MarkdownTextObject,
    SectionBlock,
)

INITIAL_CHAT_UPDATE_INTERVAL_SEC = 1

SlackRequestHandler.clear_all_log_handlers()
logging.basicConfig(
    format="%(levelname)s %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


def initialize_conversation_memory(id_ts: str) -> ConversationBufferMemory:
    """Initialize the conversation memory.

    Parameters
    ----------
    id_ts: str
        The ID timestamp.

    Returns
    -------
    ConversationBufferMemory
        The initialized conversation memory.
    """
    history = DynamoDBChatMessageHistory(
        table_name=os.environ["CHAT_HISTORY_TABLE"],
        session_id=id_ts,
    )
    return ConversationBufferMemory(
        chat_memory=history,
        memory_key="chat_history",
        return_messages=True,
    )


def initialize_vector_store() -> LanceDB:
    """Initialize the vector store.

    Returns
    -------
    LanceDB
        The initialized vector store.
    """
    db = lancedb.connect(os.environ["LANCEDB_DB"])
    table = db.open_table(os.environ["LANCEDB_TABLE"])
    embeddings = OpenAIEmbeddings()
    return LanceDB(
        table,
        embeddings,
    )


app = slack_bolt.App(
    signing_secret=os.environ["SLACK_SIGNING_SECRET"],
    token=os.environ["SLACK_BOT_TOKEN"],
    process_before_response=True,
)


class SlackStreamingCallbackHandler(BaseCallbackHandler):
    """Handles callbacks for Slack streaming."""

    last_send_time = time.time()
    message = ""

    def __init__(self: "SlackStreamingCallbackHandler", channel: str, ts: str) -> None:
        """Initialize the SlackStreamingCallbackHandler.

        Parameters
        ----------
        channel : str
            The channel to send updates to.
        ts : str
            The timestamp of the message to update.

        Returns
        -------
        None
            This is a constructor method and does not return anything.
        """
        self.channel = channel
        self.ts = ts
        self.interval = INITIAL_CHAT_UPDATE_INTERVAL_SEC
        self.update_count = 0

    def on_llm_new_token(
        self: "SlackStreamingCallbackHandler",
        token: str,
        **kwargs: Any,  # noqa: ARG002
    ) -> None:
        """Handle the new token received during the LLM process.

        Parameters
        ----------
        token : str
            The new token received.
        kwargs : Any
            Additional keyword arguments.

        Returns
        -------
        None
            Nothing to return.
        """
        self.message += token

        now = time.time()
        if now - self.last_send_time > self.interval:
            app.client.chat_update(
                channel=self.channel,
                ts=self.ts,
                text=f"{self.message}\n\nTyping...",
            )
            self.last_send_time = now
            self.update_count += 1

            if self.update_count / 10 > self.interval:
                self.interval = self.interval * 2

    def on_llm_end(
        self: "SlackStreamingCallbackHandler",
        response: LLMResult,  # noqa: ARG002
        **kwargs: Any,  # noqa: ARG002
    ) -> None:
        """Handle the end of the LLM process.

        This method is called when the LLM process is completed.

        Parameters
        ----------
        response : LLMResult
            The response of the LLM process.
        kwargs : Any
            Additional keyword arguments.

        Returns
        -------
        None
            This method does not return anything.
        """
        message_context = f"Generated by {os.environ['OPENAI_API_MODEL']}"
        message_blocks = [
            SectionBlock(text=MarkdownTextObject(text=self.message)),
            DividerBlock(),
            ContextBlock(elements=[MarkdownTextObject(text=message_context)]),
        ]

        app.client.chat_update(
            channel=self.channel,
            ts=self.ts,
            text=self.message,
            blocks=message_blocks,
        )


def handle_mention(event: dict, say: Say) -> None:
    """Handle the mention event.

    Parameters
    ----------
    event : dict
        The mention event.
    say : Say
        The say function.

    Returns
    -------
    None
        This function does not return anything.
    """
    channel = event["channel"]
    thread_ts = event["ts"]
    message = re.sub(r"<@.*>", "", event["text"]).strip()

    id_ts = event["ts"]
    if "thread_ts" in event:
        id_ts = event["thread_ts"]

    result = say("\n\nTyping...", thread_ts=thread_ts)
    ts = result["ts"]

    memory = initialize_conversation_memory(id_ts)
    vector_store = initialize_vector_store()

    slack_callback = SlackStreamingCallbackHandler(channel, ts)

    llm = ChatOpenAI(
        model=os.environ["OPENAI_API_MODEL"],
        temperature=float(os.environ["OPENAI_API_TEMPERATURE"]),
        streaming=True,
        callbacks=[slack_callback],
    )

    condense_question_llm = ChatOpenAI(
        model=os.environ["OPENAI_API_MODEL"],
        temperature=float(os.environ["OPENAI_API_TEMPERATURE"]),
    )

    cr_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(),
        memory=memory,
        condense_question_llm=condense_question_llm,
    )

    cr_chain.run(message)


def just_ack(ack: Ack) -> None:
    """Just acknowledge the request.

    Parameters
    ----------
    ack : Ack
        The acknowledgement function.

    Returns
    -------
    None
        This function does not return anything.
    """
    ack()


app.event("app_mintion")(
    ack=just_ack,
    lazy=[handle_mention],
)


def lambda_handler(event: dict, context: BoltContext) -> dict:
    """Handle the Lambda function.

    Parameters
    ----------
    event : dict
        The event dictionary.
    context : BoltContext
        The Bolt context.

    Returns
    -------
    dict
        The response dictionary.
    """
    headers = event["headers"]
    logger.info(json.dumps(headers))

    if "x-slack-retry-num" in headers:
        logger.info("SKIP > %s", headers["x-slack-retry-num"])
        return {"statusCode": 200, "body": json.dumps({"message": "ok"})}

    slack_handler = SlackRequestHandler(app=app)
    return slack_handler.handle(event, context)
